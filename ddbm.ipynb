{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from ddbm import dist_util, logger\n",
    "from datasets import load_data\n",
    "from ddbm.resample import create_named_schedule_sampler\n",
    "from ddbm.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    sample_defaults,\n",
    "    args_to_dict,\n",
    "    add_dict_to_argparser,\n",
    "    get_workdir\n",
    ")\n",
    "from ddbm.train_util import TrainLoop\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torchvision import datasets\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from datasets.augment import AugmentPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba3c43",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from torchvision import transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf24e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_statistics(dataset):\n",
    "    \"\"\"\n",
    "    Function for obtaining channel statistics (mean and deviation) for a dataset\n",
    "    \"\"\"\n",
    "    channel_sum = np.zeros(3)\n",
    "    channel_sq_sum = np.zeros(3)\n",
    "\n",
    "    num_pixels = 0\n",
    "\n",
    "    for img in tqdm(dataset):\n",
    "        img = np.array(img) / 255.0\n",
    "        channel_sum += img.mean(axis=(0, 1))\n",
    "        channel_sq_sum += (img**2).mean(axis=(0, 1))\n",
    "        num_pixels += 1\n",
    "\n",
    "    channel_mean = channel_sum / num_pixels\n",
    "    channel_std = np.sqrt(channel_sq_sum/num_pixels - channel_mean**2)\n",
    "\n",
    "    return channel_mean, channel_std\n",
    "\n",
    "\n",
    "def get_transforms(mean, std, img_size=256):\n",
    "    train_transform = tr.Compose([\n",
    "        tr.ToPILImage(),\n",
    "        tr.Resize(img_size),\n",
    "        # tr.RandomHorizontalFlip(),\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    test_transform = tr.Compose([\n",
    "        tr.ToPILImage(),\n",
    "        tr.Resize(img_size),\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    def de_normalize(tensor, normalized=True):\n",
    "        tmp = tensor.cpu() * torch.from_numpy(std).reshape(3,1,1) + torch.from_numpy(mean).reshape(3,1,1)\n",
    "        return tmp.permute(1, 2, 0)\n",
    "    \n",
    "    return train_transform, test_transform, de_normalize\n",
    "\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, mode=None):\n",
    "        \"\"\"\n",
    "        mode: one of \"a\", \"b\", None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        image = cv2.imread(img_path)[:, :, ::-1]  # Convert BGR to RGB\n",
    "        if self.mode == \"a\":\n",
    "            image = image[:, :image.shape[1]//2, :]\n",
    "        elif self.mode == \"b\":\n",
    "            image = image[:, image.shape[1]//2:, :]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform_a=None, transform_b=None, random_flip=None, random_crop=None):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = [os.path.join(root_dir, f) for f in os.listdir(root_dir)]\n",
    "        self.transform_a = transform_a\n",
    "        self.transform_b = transform_b\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_paths[index]\n",
    "        image = cv2.imread(img_path)[:, :, ::-1]  # Convert BGR to RGB\n",
    "        image_a = image[:, :image.shape[1]//2, :]\n",
    "        image_b = image[:, image.shape[1]//2:, :]\n",
    "        if self.transform_a:\n",
    "            image_a = self.transform_a(image_a)\n",
    "        if self.transform_b:\n",
    "            image_b = self.transform_b(image_b)\n",
    "        return image_a, image_b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ImagesDatasetsClass:\n",
    "    train_a: PairedDataset\n",
    "    train_b: PairedDataset\n",
    "    val_a: PairedDataset\n",
    "    val_b: PairedDataset\n",
    "\n",
    "@dataclass\n",
    "class PairedDatasetsClass:\n",
    "    train: PairedDataset\n",
    "    val: PairedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79068bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        data_dir=\"\",\n",
    "        dataset='edges2handbags',\n",
    "        schedule_sampler=\"uniform\",\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.0,\n",
    "        lr_anneal_steps=0,\n",
    "        global_batch_size=2048,\n",
    "        batch_size=-1,\n",
    "        microbatch=-1,  # -1 disables microbatches\n",
    "        ema_rate=\"0.9999\",  # comma-separated list of EMA values\n",
    "        log_interval=50,\n",
    "        test_interval=500,\n",
    "        save_interval=10000,\n",
    "        save_interval_for_preemption=50000,\n",
    "        resume_checkpoint=\"\",\n",
    "        exp='',\n",
    "        use_fp16=False,\n",
    "        fp16_scale_growth=1e-3,\n",
    "        debug=False,\n",
    "        num_workers=2,\n",
    "        use_augment=False\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_dict_to_argparser(parser, defaults)\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = create_argparser().parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef950b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a diffusion model on images.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    workdir = get_workdir(args.exp)\n",
    "    Path(workdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    dist_util.setup_dist()\n",
    "    logger.configure(dir=workdir)\n",
    "    if dist.get_rank() == 0:\n",
    "        name = args.exp if args.resume_checkpoint == \"\" else args.exp + '_resume'\n",
    "        wandb.init(project=\"bridge\", group=args.exp,name=name, config=vars(args), mode='online' if not args.debug else 'disabled')\n",
    "        logger.log(\"creating model and diffusion...\")\n",
    "    \n",
    "\n",
    "    data_image_size = args.image_size\n",
    "    \n",
    "\n",
    "    if args.resume_checkpoint == \"\":\n",
    "        model_ckpts = list(glob(f'{workdir}/*model*[0-9].*'))\n",
    "        if len(model_ckpts) > 0:\n",
    "            max_ckpt = max(model_ckpts, key=lambda x: int(x.split('model_')[-1].split('.')[0]))\n",
    "            if os.path.exists(max_ckpt):\n",
    "                args.resume_checkpoint = max_ckpt\n",
    "                if dist.get_rank() == 0:\n",
    "                    logger.log('Resuming from checkpoint: ', max_ckpt)\n",
    "\n",
    "\n",
    "    model, diffusion = create_model_and_diffusion(\n",
    "        **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
    "    )\n",
    "    model.to(dist_util.dev())\n",
    "\n",
    "    if dist.get_rank() == 0:\n",
    "        wandb.watch(model, log='all')\n",
    "    schedule_sampler = create_named_schedule_sampler(args.schedule_sampler, diffusion)\n",
    "\n",
    "    \n",
    "    if args.batch_size == -1:\n",
    "        batch_size = args.global_batch_size // dist.get_world_size()\n",
    "        if args.global_batch_size % dist.get_world_size() != 0:\n",
    "            logger.log(\n",
    "                f\"warning, using smaller global_batch_size of {dist.get_world_size()*batch_size} instead of {args.global_batch_size}\"\n",
    "            )\n",
    "    else:\n",
    "        batch_size = args.batch_size\n",
    "        \n",
    "    if dist.get_rank() == 0:\n",
    "        logger.log(\"creating data loader...\")\n",
    "\n",
    "    data, test_data = load_data(\n",
    "        data_dir=args.data_dir,\n",
    "        dataset=args.dataset,\n",
    "        batch_size=batch_size,\n",
    "        image_size=data_image_size,\n",
    "        num_workers=args.num_workers,\n",
    "    )\n",
    "    \n",
    "    if args.use_augment:\n",
    "        augment = AugmentPipe(\n",
    "                p=0.12,xflip=1e8, yflip=1, scale=1, rotate_frac=1, aniso=1, translate_frac=1\n",
    "            )\n",
    "    else:\n",
    "        augment = None\n",
    "        \n",
    "    logger.log(\"training...\")\n",
    "    TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        train_data=data,\n",
    "        test_data=test_data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=args.microbatch,\n",
    "        lr=args.lr,\n",
    "        ema_rate=args.ema_rate,\n",
    "        log_interval=args.log_interval,\n",
    "        test_interval=args.test_interval,\n",
    "        save_interval=args.save_interval,\n",
    "        save_interval_for_preemption=args.save_interval_for_preemption,\n",
    "        resume_checkpoint=args.resume_checkpoint,\n",
    "        workdir=workdir,\n",
    "        use_fp16=args.use_fp16,\n",
    "        fp16_scale_growth=args.fp16_scale_growth,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=args.weight_decay,\n",
    "        lr_anneal_steps=args.lr_anneal_steps,\n",
    "        augment_pipe=augment,\n",
    "        **sample_defaults()\n",
    "    ).run_loop()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
